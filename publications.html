<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Shuai Zhang Postdoc@RPI </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Shuai@RPI</div>
<div class="menu-item"><a href="index.html" >Home</a></div>
<div class="menu-item"><a href="cv\CV.pdf">CV</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-category">Others</div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p>(See the <b>full publication</b> list at <a href="https://scholar.google.com/citations?user=RvDk-iwAAAAJ&hl=en">Google scholar</a>)</p>
<h2>Selected Publications</h2>
<p>* represents equal contribution</p>
 <ol>
<li><p><b>Shuai Zhang</b>, Meng Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miao Liu. “<em>Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks.</em>” In International Conference on Learning Representations (<b>ICLR</b>), 2023. [<a href="Pub/ICLR23.pdf">pdf</a>]</p>
</li>
 <li><p><b>Shuai Zhang</b>, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. “<em>How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis</em>.” In Proc. of The Tenth International Conference on Learning Representations (<b>ICLR</b>), 2022. [<a href="Pub/ICLR22.pdf">pdf</a>]</p>
</li>
  
<li><p><b>Shuai Zhang</b>, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. “<em>Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Sparse Neural Networks.</em>” In Proc. of the Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2021. [<a href="Pub/NeurIPS21.pdf">pdf</a>]</p>
</li>
  
<li><p><b>Shuai Zhang</b>, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. “<em>Fast Learning of Graph Neural Networks with Guaranteed Generalizability: Onehidden-layer Case.</em>” In Proc. of 2020 International Conference on Machine Learning (<b>ICML</b>), pp. 11268-11277. PMLR, 2020. [<a href="Pub/ICML20.pdf">pdf</a>]</p>
</li>
  
<li><p><b>Shuai Zhang</b>, MengWang, JinjunXiong, Sijia Liu, and Pin-Yu Chen. “<em>Improved Linear Convergence of Training CNNs With Generalizability Guarantees: A One-Hidden-Layer Case.</em>” IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>). IEEE, 2020. [<a href="Pub/TNNLS.pdf">pdf</a>]</p>
</li>
  
<li><p><b>Shuai Zhang</b>, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Shuai Zhang, and Meng Wang. “<em>Correction of corrupted columns through fast robust Hankel matrix completion.</em>” IEEE Transactions on Signal Processing (<b>TSP</b>), no. 10: 2580-2594. IEEE, 2019. [<a href="Pub/TSP2.pdf">pdf</a>]</p>
</li>
  
<li><p><b>Shuai Zhang</b>, Yingshuai Hao, Meng Wang, and Joe H. Chow. “<em>Multichannel Hankel matrix completion through nonconvex optimization.</em>” IEEE Journal of Selected Topics in Signal Processing (<b>JSTSP</b>), no. 4: 617-632. IEEE, 2018. [<a href="Pub/TSP1.pdf">pdf</a>]</p>  
</li>  
  
<li><p><b>Shuai Zhang</b>, and Meng Wang. “<em>Correction of simultaneous bad measurements by exploiting the low-rank hankel structure.</em>” In 2018 IEEE International Symposium on Information Theory (<b>ISIT</b>), pp. 646-650. IEEE, 2018.</p>  
</li>  
  
<li><p>Hongkang Li, <b>Shuai Zhang</b>, Meng Wang. “<em>Learning and generalization of onehidden-layer neural networks, going beyond standard Gaussian data.</em>” In 2022 56th Annual Conference on Information Sciences and Systems (<b>CISS</b>), pp. 1-6. IEEE, 2022.</p>  
</li>  
  
<li><p><b>Shuai Zhang</b>, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. “<em>Guaranteed Convergence of Training Convolutional Neural Networks via Accelerated Gradient Descent.</em>” In 2020 54th Annual Conference on Information Sciences and Systems (<b>CISS</b>), pp. 1-6. IEEE, 2020.</p>  
</li>  

<li><p><b>Shuai Zhang</b>, Yingshuai Hao, Meng Wang, and Joe H. Chow. “<em>Multi-channel missing data recovery by exploiting the low-rank hankel structures.</em>” In 2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (<b>CAMSAP</b>), pp. 1-5. IEEE, 2017.</p>  
</li>    
</ol>
  
<h2>Preprint</h2>  
<ol>
<li><p><b>Shuai Zhang</b>, Hongkang Li, Meng Wang, Miao Liu, Pin-Yu Chen, Songtao Lu, Sijia Liu, Keerthiram Murugesan, Subhajit Chaudhury. “<em>On the Convergence and Sample Complexity Analysis of Deep Q-Networks with Epsilon-Greedy Exploration.</em>” Submitted to The Fortieth International Conference on Machine Learning (<b>ICML</b>), 2023.
</li>
  
  <li><p>Hongkang Li, <b>Shuai Zhang</b>, Meng Wang, Yihua Zhang, Pin-Yu Chen, Sijia Liu. “<em>Does promoting the minority group always improve generalization? A theoretical study of one-hidden-layer neural networks on group imbalance.</em>” Submitted to The Conference on Uncertainty in Artificial Intelligence (<b>UAI</b>), 2023.
</li>
  
  <li><p>Nowaz Chowdhury, <b>Shuai Zhang</b>, Meng Wang, Sijia Liu, Pin-Yu Chen. “<em>Patchlevel routing in mixture of experts is provably sample-efficient for convolutional neural networks.</em>” Submitted to The Fortieth International Conference on Machine Learning (<b>ICML</b>), 2023.
</li>    
    
</ol>
</td>
</tr>
</table>
</body>
</html>
